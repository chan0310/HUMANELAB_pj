{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer.Config import Config\n",
    "from Transformer.Model import Transformer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from Seq2SeqModel.Seq2SeqModel import Seq2SeqModel\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# CSV 파일에서 데이터 불러오기\n",
    "df_train = pd.read_csv(\"Datatset3/train.csv\")\n",
    "df_val = pd.read_csv(\"Datatset3/val.csv\")\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"Datatset3/tokenizer\")\n",
    "\n",
    "def str_to_list(input_string):\n",
    "    cleaned_string = input_string.strip(\"[]\").replace(\" \", \"\")\n",
    "\n",
    "    number_strings = cleaned_string.split(\",\")\n",
    "    number_list = [int(num) for num in number_strings]\n",
    "    return number_list\n",
    "\n",
    "# 데이터 추출\n",
    "X_train = df_train[\"X_train\"].values.tolist()\n",
    "X_train=[str_to_list(i) for i in X_train]\n",
    "y_train = df_train[\"y_train\"].values.tolist()\n",
    "y_train=[str_to_list(i) for i in y_train]\n",
    "\n",
    "\n",
    "X_val = df_val[\"X_val\"].values.tolist()\n",
    "X_val=[str_to_list(i) for i in X_val]\n",
    "y_val = df_val[\"y_val\"].values.tolist()\n",
    "y_val=[str_to_list(i) for i in y_val]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def to_unk_dset(dataset_2d,vocab):\n",
    "    # 데이터셋에서 모든 센텐스를 하나의 1차원 리스트로 펼칩니다.\n",
    "    flat_sentences = [token for sentence in dataset_2d for token in sentence]\n",
    "\n",
    "    # 단어 빈도를 계산합니다.\n",
    "    word_counts = Counter(flat_sentences)\n",
    "\n",
    "    # 등장 빈도가 낮은 단어를 찾아 <unk>로 대체하고 vocab에서 제거합니다.\n",
    "    min_word_frequency = 2  # 등장 빈도가 이 값보다 작으면 <unk>로 대체\n",
    "    filtered_sentences = []\n",
    "    for sentence in dataset_2d:\n",
    "        filtered_sentence = [token if word_counts[token] >= min_word_frequency else '<unk>' for token in sentence]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    # vocab에서 <unk>를 제거합니다.\n",
    "    vocab = list(set(flat_sentences))\n",
    "    if '<unk>' in vocab:\n",
    "        vocab.remove('<unk>')\n",
    "\n",
    "    # 처리된 데이터셋과 vocab을 사용하여 모델을 학습하거나 다른 작업을 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "print(f'Using {device}')\n",
    "\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"Datatset/tokenizer\")\n",
    "config=Config(3)\n",
    "model=Transformer(config)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-11)\n",
    "batchsize=64\n",
    "print(\"Set\")\n",
    "module=Seq2SeqModel( model,tokenizer,optimizer,loss_fn,\n",
    "                    X_train,y_train,X_val,y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3   Batch: 5/5   cost: 0.0103371: |████████████████    |80.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Running_Loss: 0.01 VAL_ACC: 0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|                    |0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m module\u001b[39m.\u001b[39;49mtrain_main(\u001b[39m3\u001b[39;49m,\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/HUMANE lab/project/transformer_impl/Seq2SeqModel/Seq2SeqModel.py:91\u001b[0m, in \u001b[0;36mSeq2SeqModel.train_main\u001b[0;34m(self, max_epoch, check)\u001b[0m\n\u001b[1;32m     89\u001b[0m x2\u001b[39m=\u001b[39mx2\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     90\u001b[0m yy\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mzeros(yy\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m),yy\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_dec_vocab)\u001b[39m.\u001b[39mscatter_(\u001b[39m2\u001b[39m, yy\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m yy\u001b[39m=\u001b[39myy\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     92\u001b[0m current_loss,attentions\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_each_batch(x1,x2,yy)\n\u001b[1;32m     93\u001b[0m running_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mcurrent_loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "module.train_main(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> ▁Calibration is▁about to check the▁value▁range your▁device▁delivers.▁Please▁move▁axis %1 %2 on your▁device to the maximum position. Press▁any▁button on the▁device or▁click on the'Next 'button to continue with the▁next▁step.\n",
      "<bos> Le calibrage va vérifier la plage de valeurs que votre matériel produit. Veuillez déplacer l'axe %1 %2 de votre périphérique à la position maximale. Appuyez sur n'importe quel bouton du périphérique ou sur le bouton « & #160; Suivant & #160; » pour la prochaine étape.</s>\n",
      "[59514, 34378, 226, 5783, 32, 200, 12, 3647, 4, 1223, 1628, 117, 4923, 23608, 3, 1789, 2942, 20059, 301, 548, 301, 331, 30, 117, 4923, 12, 4, 1528, 668, 3, 5734, 212, 9319, 30, 4, 4923, 57, 5487, 30, 4, 6, 32712, 25, 7243, 1160, 12, 621, 42, 4, 1156, 3009, 3, 0]\n",
      "[59514, 60, 7418, 5244, 8234, 740, 4993, 8, 6471, 5, 2218, 29, 193, 2220, 742, 3, 4366, 14237, 14, 6, 16600, 301, 548, 301, 331, 5, 193, 24275, 17, 8, 668, 6142, 3, 33640, 36, 81, 6, 5411, 2709, 9376, 22, 24275, 59, 36, 19, 9376, 153, 402, 29033, 13774, 402, 29033, 416, 27, 8, 4034, 4888, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(X_train[0])))\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(y_train[0])))\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_padd:  128\n",
      "seq_padd:  128\n",
      "<bos> ▁Calibration is▁about to check the▁value▁range your▁device▁delivers.▁Please▁move▁axis %1 %2 on your▁device to the maximum position. Press▁any▁button on the▁device or▁click on the'Next 'button to continue with the▁next▁step.\n",
      "tensor([17833,  2332,  2332,  2332, 17154, 17154, 51385, 51385, 51385, 17154,\n",
      "        17154, 17154, 51385, 51385, 25337, 17154, 17154, 40291, 40291, 21398,\n",
      "        33386,  4875, 51385, 51385, 51385, 51385, 30212, 30212, 30212, 30212,\n",
      "        30212, 51385, 51385, 51385, 51385, 51385, 51385, 51385, 51385, 51385,\n",
      "        50510, 50510, 50510, 51385, 51385, 51385, 51671, 50983, 21398, 21398,\n",
      "        21398, 50510, 49137, 49137, 50983, 51385, 51385, 51385, 25337, 25337,\n",
      "        25337, 51385, 51385,   860,   860,   860, 47255, 47255, 51671, 51671,\n",
      "        51671, 51671, 51385, 51385, 51385, 51385, 51385, 51385, 40269, 40269,\n",
      "        51385, 51385, 51385, 45835, 45835, 47255, 47255, 47255, 51385, 51385,\n",
      "        49137,   924, 47255, 47255, 51385, 51385, 44622, 12016, 12016, 12016,\n",
      "          860,   860,   860,   860,   860,   860, 47255, 47255, 47255, 29964,\n",
      "        45921, 51671, 51671, 51671, 47255, 47255, 47255, 47255, 47255, 47255,\n",
      "        47255, 47255, 47255, 47255, 47255, 47255, 21398, 21398]) torch.Size([128])\n",
      "drôle torture torture tortureACIAACIA扣扣扣ACIAACIAACIA扣扣▁RacismACIAACIAinsurrectioninsurrection▁Exhibitaoui respective扣扣扣扣 pittoresque pittoresque pittoresque pittoresque pittoresque扣扣扣扣扣扣扣扣扣项项项扣扣扣흔적票▁Exhibit▁Exhibit▁Exhibit项∧∧票扣扣扣▁Racism▁Racism▁Racism扣扣▁political▁political▁political▁organisers▁organisers흔적흔적흔적흔적扣扣扣扣扣扣 correspondait correspondait扣扣扣traffickingtrafficking▁organisers▁organisers▁organisers扣扣∧ beaucoup▁organisers▁organisers扣扣▁devastated douane douane douane▁political▁political▁political▁political▁political▁political▁organisers▁organisers▁organisers▁0.8▁Episode흔적흔적흔적▁organisers▁organisers▁organisers▁organisers▁organisers▁organisers▁organisers▁organisers▁organisers▁organisers▁organisers▁organisers▁Exhibit▁Exhibit\n"
     ]
    }
   ],
   "source": [
    "module.model.eval()\n",
    "i_e=torch.tensor(X_train[0][:-1])\n",
    "out=torch.tensor(module.seq_to_seq_process(i_e))\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(i_e)))\n",
    "print(out,out.shape)\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
