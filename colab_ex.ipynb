{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from Seq2SeqModel.Seq2SeqModel import Seq2SeqModel\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.init as init\n",
    "\n",
    "# CSV 파일에서 데이터 불러오기\n",
    "df_train = pd.read_csv(\"Datatset2/train.csv\")\n",
    "df_val = pd.read_csv(\"Datatset2/val.csv\")\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"Datatset/tokenizer\")\n",
    "\n",
    "def str_to_list(input_string):\n",
    "    cleaned_string = input_string.strip(\"[]\").replace(\" \", \"\")\n",
    "\n",
    "    number_strings = cleaned_string.split(\",\")\n",
    "    number_list = [int(num) for num in number_strings]\n",
    "    return number_list\n",
    "\n",
    "# 데이터 추출\n",
    "X_train = df_train[\"X_train\"].values.tolist()\n",
    "X_train=[str_to_list(i) for i in X_train]\n",
    "y_train = df_train[\"y_train\"].values.tolist()\n",
    "y_train=[str_to_list(i) for i in y_train]\n",
    "\n",
    "\n",
    "X_val = df_val[\"X_val\"].values.tolist()\n",
    "X_val=[str_to_list(i) for i in X_val]\n",
    "y_val = df_val[\"y_val\"].values.tolist()\n",
    "y_val=[str_to_list(i) for i in y_val]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set\n",
      "Using cpu\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "tokenizer= AutoTokenizer.from_pretrained(\"Datatset/tokenizer\")\n",
    "model=None\n",
    "class_weights = torch.ones(tokenizer.vocab_size+1).to(device=device)\n",
    "class_weights[tokenizer.pad_token_id]=0.1\n",
    "class_weights[tokenizer.pad_token_id-1]=0.1\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.01,weight=class_weights)\n",
    "optimizer=None\n",
    "batchsize=64\n",
    "lr=0.005\n",
    "print(\"Set\")\n",
    "module=Seq2SeqModel( tokenizer,loss_fn,lr,\n",
    "                    X_train,y_train,X_val,y_val,batch_size=batchsize,device=torch.device(\"cpu\"))\n",
    "print(module.config.n_dec_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"train\")\n",
    "module.load_model(\"saved56.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 59515, 'n_dec_vocab': 59515, 'n_enc_seq': 128, 'n_dec_seq': 128, 'n_layer': 4, 'd_hidn': 128, 'i_pad': 0, 'd_ff': 256, 'n_head': 3, 'd_head': 128, 'dropout': 0.2, 'layer_norm_epsilon': 1e-12}\n"
     ]
    }
   ],
   "source": [
    "print(module.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a,b)=(torch.tensor(X_train[4]),torch.tensor(y_train[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([59514,   135,   607,  2054,     2,  3482,    10,  2843, 21048,    26,\n",
      "           67,   478,     0]) tensor([59514,  4322,     5, 30508,     2,     5,  4403,    11,     5,  6676,\n",
      "           27,    66,   478,     0]) 59513\n"
     ]
    }
   ],
   "source": [
    "print(a,b,tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_padd:  128\n",
      "seq_padd:  128\n",
      "seq_padd:  127\n"
     ]
    }
   ],
   "source": [
    "out,*inputs=module.seq_to_seq_process(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> Sentence,▁word and▁letter▁counts for this document</s>\n",
      "<bos> Nombre de phrases, de mots et de lettres pour ce document\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens([i for i in inputs[0] if i!=tokenizer.pad_token_id])))\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens([i for i in inputs[1] if i!=tokenizer.pad_token_id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'& &'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens([i for i in out if i!=tokenizer.pad_token_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
