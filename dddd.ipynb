{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer.Config import Config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Transformer.Trainer.Tokenizer import TokenizerPlus\n",
    "from Transformer.Trainer.Trainer import Trainer\n",
    "from Transformer.Trainer.decoding import greedy_decoding\n",
    "from Transformer.Config import Config\n",
    "from Transformer.Model import Transformer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'New Action', 'fr': 'Nouvelle action'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][3][\"translation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/pytorchVenv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/pytorchVenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3635: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(fr_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']\n",
      "['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # 타겟을 위한 토크나이저 셋업\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "validation_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "X_train = train_dataset[\"input_ids\"]\n",
    "y_train = train_dataset[\"labels\"]\n",
    "\n",
    "X_val = validation_dataset[\"input_ids\"]\n",
    "y_val = validation_dataset[\"labels\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189155, 128]) torch.Size([189155, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "X_train = [torch.tensor(seq) for seq in X_train]\n",
    "y_train = [torch.tensor(seq) for seq in y_train]\n",
    "X_train= pad_sequence([seq.flip(0) for seq in X_train], batch_first=True, padding_value=0).flip(1)\n",
    "y_train= pad_sequence([seq.flip(0) for seq in y_train], batch_first=True, padding_value=0).flip(1)\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21018, 128]) torch.Size([21018, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_val = [torch.tensor(seq) for seq in X_val]\n",
    "y_val = [torch.tensor(seq) for seq in y_val]\n",
    "X_val= pad_sequence([seq.flip(0) for seq in X_val], batch_first=True, padding_value=0).flip(1)\n",
    "y_val= pad_sequence([seq.flip(0) for seq in y_val], batch_first=True, padding_value=0).flip(1)\n",
    "print(X_val.shape,y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59514"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 59515, 'n_dec_vocab': 59515, 'n_enc_seq': 128, 'n_dec_seq': 128, 'n_layer': 4, 'd_hidn': 128, 'i_pad': 0, 'd_ff': 256, 'n_head': 4, 'd_head': 128, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
     ]
    }
   ],
   "source": [
    "n=128\n",
    "config=Config(len(tokenizer.get_vocab())+1)\n",
    "config.n_enc_seq=n\n",
    "config.n_dec_seq=n\n",
    "config.d_hidn=n\n",
    "config.d_ff=n*2\n",
    "config.d_head=n\n",
    "config.n_layer=4\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (emb): Embeding(\n",
       "      (emb): Embedding(59515, 128)\n",
       "      (pos_emb): Embedding(129, 128)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (W_K): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (W_V): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (scaled_dot_attn): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (pos_ffn): PoswiseFeedForwardNet(\n",
       "          (conv1): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): Embeding(\n",
       "      (emb): Embedding(59515, 128)\n",
       "      (pos_emb): Embedding(129, 128)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (W_K): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (W_V): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (scaled_dot_attn): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dec_enc_attn): MultiHeadAttention(\n",
       "          (W_Q): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (W_K): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (W_V): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (scaled_dot_attn): ScaledDotProductAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (pos_ffn): PoswiseFeedForwardNet(\n",
       "          (conv1): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "          (conv2): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Linear(in_features=128, out_features=59515, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(config)  # 모델 클래스를 인스턴스화\n",
    "model.load_state_dict(torch.load('complete_model.pth'))\n",
    "model.eval() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([189155, 128])\n",
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0].shape)\n",
    "print(X_train[0].view(1,-1).shape)\n",
    "print(X_train.shape)\n",
    "print(X_train.view(X_train.size(0),1,-1)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "0/\n",
      "----\n",
      "1/\n",
      "----\n",
      "2/\n",
      "----\n",
      "3/\n",
      "----\n",
      "4/\n",
      "----\n",
      "5/\n",
      "----\n",
      "6/\n",
      "----\n",
      "7/\n",
      "----\n",
      "8/\n",
      "----\n",
      "9/\n",
      "----\n",
      "10/\n",
      "----\n",
      "11/\n",
      "----\n",
      "12/\n",
      "----\n",
      "13/\n",
      "----\n",
      "14/\n",
      "----\n",
      "15/\n",
      "----\n",
      "16/\n",
      "----\n",
      "17/\n",
      "----\n",
      "18/\n",
      "----\n",
      "19/\n",
      "----\n",
      "20/\n",
      "----\n",
      "21/\n",
      "----\n",
      "22/\n",
      "----\n",
      "23/\n",
      "----\n",
      "24/\n",
      "----\n",
      "25/\n",
      "----\n",
      "26/\n",
      "----\n",
      "27/\n",
      "----\n",
      "28/\n",
      "----\n",
      "29/\n",
      "----\n",
      "30/\n",
      "----\n",
      "31/\n",
      "----\n",
      "32/\n",
      "----\n",
      "33/\n",
      "----\n",
      "34/\n",
      "----\n",
      "35/\n",
      "----\n",
      "36/\n",
      "----\n",
      "37/\n",
      "----\n",
      "38/\n",
      "----\n",
      "39/\n",
      "----\n",
      "40/\n",
      "----\n",
      "41/\n",
      "----\n",
      "42/\n",
      "----\n",
      "43/\n",
      "----\n",
      "44/\n",
      "----\n",
      "45/\n",
      "----\n",
      "46/\n",
      "----\n",
      "47/\n",
      "----\n",
      "48/\n",
      "----\n",
      "49/\n",
      "----\n",
      "50/\n",
      "----\n",
      "51/\n",
      "----\n",
      "52/\n",
      "----\n",
      "53/\n",
      "----\n",
      "54/\n",
      "----\n",
      "55/\n",
      "----\n",
      "56/\n",
      "----\n",
      "57/\n",
      "----\n",
      "58/\n",
      "----\n",
      "59/\n",
      "----\n",
      "60/\n",
      "----\n",
      "61/\n",
      "----\n",
      "62/\n",
      "----\n",
      "63/\n",
      "----\n",
      "64/\n",
      "----\n",
      "65/\n",
      "----\n",
      "66/\n",
      "----\n",
      "67/\n",
      "----\n",
      "68/\n",
      "----\n",
      "69/\n",
      "----\n",
      "70/\n",
      "----\n",
      "71/\n",
      "----\n",
      "72/\n",
      "----\n",
      "73/\n",
      "----\n",
      "74/\n",
      "----\n",
      "75/\n",
      "----\n",
      "76/\n",
      "----\n",
      "77/\n",
      "----\n",
      "78/\n",
      "----\n",
      "79/\n",
      "----\n",
      "80/\n",
      "----\n",
      "81/\n",
      "----\n",
      "82/\n",
      "----\n",
      "83/\n",
      "----\n",
      "84/\n",
      "----\n",
      "85/\n",
      "----\n",
      "86/\n",
      "----\n",
      "87/\n",
      "----\n",
      "88/\n",
      "----\n",
      "89/\n",
      "----\n",
      "90/\n",
      "----\n",
      "91/\n",
      "----\n",
      "92/\n",
      "----\n",
      "93/\n",
      "----\n",
      "94/\n",
      "----\n",
      "95/\n",
      "----\n",
      "96/\n",
      "----\n",
      "97/\n",
      "----\n",
      "98/\n",
      "----\n",
      "99/\n",
      "----\n",
      "100/\n",
      "----\n",
      "101/\n",
      "----\n",
      "102/\n",
      "----\n",
      "103/\n",
      "----\n",
      "104/\n",
      "----\n",
      "105/\n",
      "----\n",
      "106/\n",
      "----\n",
      "107/\n",
      "----\n",
      "108/\n",
      "----\n",
      "109/\n",
      "----\n",
      "110/\n",
      "----\n",
      "111/\n",
      "----\n",
      "112/\n",
      "----\n",
      "113/\n",
      "----\n",
      "114/\n",
      "----\n",
      "115/\n",
      "----\n",
      "116/\n",
      "----\n",
      "117/\n",
      "----\n",
      "118/\n",
      "----\n",
      "119/\n",
      "----\n",
      "120/\n",
      "----\n",
      "121/\n",
      "----\n",
      "122/\n",
      "----\n",
      "123/\n",
      "----\n",
      "124/\n",
      "----\n",
      "125/\n",
      "----\n",
      "126/\n",
      "----\n",
      "127/"
     ]
    }
   ],
   "source": [
    "r=greedy_decoding(model, X_train.view(X_train.size(0),1,-1).to(torch.int64)[0] ,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[817,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0, 34378,   226,  5783,    32,\n",
      "           200,    12,  3647,     4,  1223,  1628,   117,  4923, 23608,     3,\n",
      "          1789,  2942, 20059,   301,   548,   301,   331,    30,   117,  4923,\n",
      "            12,     4,  1528,   668,     3,  5734,   212,  9319,    30,     4,\n",
      "          4923,    57,  5487,    30,     4,     6, 32712,    25,  7243,  1160,\n",
      "            12,   621,    42,     4,  1156,  3009,     3,     0]])\n"
     ]
    }
   ],
   "source": [
    "print( X_train[0].view(1,-1).to(torch.int64))\n",
    "x= X_train[0].view(1,-1).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mtensor(tokenizer\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(x[\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "torch.tensor(tokenizer.convert_ids_to_tokens(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0, 10773,    20,     6,  1549,\n",
      "             5,    14,     6,  8543,    11,    22,   644,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0, 42691,   108,    19,  2454,   738,     0],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,   335, 15973,  3435,    63,    34,\n",
      "          1574, 16829,    17,    14,     6, 29180,     3,  9538,  1648, 16036,\n",
      "           139,   110,    27, 33614,    14,     6, 18412,     5,    66, 15973,\n",
      "             3, 16468,   265,    68,     6,   107,    43,  4772,    27,     8,\n",
      "          1565,    13,  7907,     9,  3397,    20,     6,  4482,   497,   936,\n",
      "            27, 38892,   810,    16, 32239,     9,     3,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(y_val[0:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
