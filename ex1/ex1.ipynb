{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Transformermy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mTransformermy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mConfig\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mTransformermy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mModel\u001b[39;00m \u001b[39mimport\u001b[39;00m Transformer\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msentencepiece\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspm\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Transformermy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Transformer.Config import Config\n",
    "from Transformer.Model import Transformer\n",
    "import sentencepiece as spm\n",
    "import tensorflow_datasets as tfds\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "\n",
    "df=pd.read_csv(\"ex1.csv\")\n",
    "\n",
    "questions=[i for i in df[\"q\"]]\n",
    "answers=[i for i in df[\"a\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "# ﻿﻿《밑바닥부터 시작하는 딥러닝2》역자 깃허브에서 데이터를 가져옵니다.\n",
    "url = \"https://raw.githubusercontent.com/WegraLee/deep-learning-from-scratch-2/master/dataset/date.txt\"\n",
    "r = requests.get(url)\n",
    "\n",
    "questions, answers = [], []\n",
    "for line in r.text.strip().split('\\n'):\n",
    "    idx = line.find('_')\n",
    "    questions.append(line[:idx].strip())\n",
    "    answers.append(line[idx:].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "class TokenizerPlus(Tokenizer):\n",
    "        \n",
    "    def to_tensor(self, texts, **kwargs):\n",
    "        sequences = self.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(sequences, **kwargs)\n",
    "        return torch.tensor(padded, dtype=torch.int64)\n",
    "    \n",
    "    def to_string(self, tensor):\n",
    "        texts = self.sequences_to_texts(tensor.data.numpy())\n",
    "        return [t[::2] for t in texts]\n",
    "    \n",
    "\n",
    "# 토큰으로 나눠주고 패딩 처리를 합니다.\n",
    "tokenizer = TokenizerPlus(char_level=True, filters='')\n",
    "tokenizer.fit_on_texts(questions)\n",
    "tokenizer.fit_on_texts(answers)\n",
    "\n",
    "src = tokenizer.to_tensor(questions)\n",
    "tgt = tokenizer.to_tensor(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 29]) torch.Size([50000, 11])\n"
     ]
    }
   ],
   "source": [
    "print(src.shape,tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(trm, src, start_token):\n",
    "    N = src.size(0)\n",
    "    preds = tokenizer.to_tensor([start_token]*N)\n",
    "\n",
    "    for _ in range(10):\n",
    "        y_pred,a,b,c = trm(src,preds)\n",
    "        t_pred = torch.argmax(y_pred[:,-1,:], axis=-1, keepdims=True)\n",
    "        preds = torch.cat([preds, t_pred], axis=1)            \n",
    "    \n",
    "    return tokenizer.to_string(preds[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, pad_id=0, start_token='_'):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.pad_id = pad_id\n",
    "        self.start_token = start_token\n",
    "        self.losses = []\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def train(self, src, tgt, max_epoch=1, batch_size=64):\n",
    "        X1_train = src\n",
    "        X2_train = tgt[:, :-1]\n",
    "        y_train = tgt[:, 1:]\n",
    "        ds = TensorDataset(X1_train, X2_train, y_train)\n",
    "        loader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(1, max_epoch+1):\n",
    "            self.model.train()\n",
    "            running_loss = 0\n",
    "            self.current_epoch += 1\n",
    "            print(\"EPOCH: %s :: \" %self.current_epoch, end='')\n",
    "            for i, (x1, x2, yy) in enumerate(loader):\n",
    "                if(i%8==0):\n",
    "                    print(i,end=\"/\")\n",
    "                y_pred,ea,de,eda = self.model(x1, x2)\n",
    "                loss = self.loss_fn(y_pred.view(-1, y_pred.size(-1)), yy.view(-1))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            self.model.eval()\n",
    "            print(\"eval\")\n",
    "            running_loss = round(running_loss / (i+1), 3)\n",
    "            print(\"running loss\")\n",
    "            self.losses.append(running_loss)\n",
    "            print(\"losses\")\n",
    "            acc = self.evaluate(src[:20], y_train[:20])\n",
    "            print(\"Loss: %s\" %(running_loss), \"ACC: %s\" %acc)\n",
    "\n",
    "    def evaluate(self, src, y):\n",
    "        pred = np.array(greedy_decoding(self.model,src, start_token='_'))\n",
    "        y_text = np.array(tokenizer.to_string(y))\n",
    "        acc = (pred == y_text).sum() / y_text.size\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Transformermy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mTransformermy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mConfig\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n\u001b[1;32m      2\u001b[0m config\u001b[39m=\u001b[39mConfig(\u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mword_index)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m config\u001b[39m.\u001b[39mn_enc_seq\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Transformermy'"
     ]
    }
   ],
   "source": [
    "\n",
    "from Transformer.Config import Config\n",
    "config=Config(len(tokenizer.word_index)+1)\n",
    "config.n_enc_seq=64\n",
    "config.n_dec_seq=64\n",
    "config.d_hidn=64\n",
    "config.d_ff=128\n",
    "config.d_head=64\n",
    "\n",
    "config.n_layer=2\n",
    "print(config)\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([50000, 29]) torch.Size([50000, 11])\n"
     ]
    }
   ],
   "source": [
    "print(type(src),(type(tgt)))\n",
    "print(src.shape,tgt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 :: 0/8/16/24/32/40/48/56/64/72/80/88/96/104/112/120/128/136/144/152/160/168/176/184/192/200/208/216/224/232/240/248/256/264/272/280/288/296/304/312/320/328/336/344/352/360/368/376/384/392/400/408/416/424/432/440/448/456/464/472/480/488/496/504/512/520/528/536/544/552/560/568/576/584/592/600/608/616/624/632/640/648/656/664/672/680/688/696/704/712/720/728/736/744/752/760/768/776/eval\n",
      "running loss\n",
      "losses\n",
      "Loss: 0.116 ACC: 0.95\n"
     ]
    }
   ],
   "source": [
    "from Transformer.Model import Transformer\n",
    "config=Config(len(tokenizer.word_index)+1)\n",
    "\n",
    "model = Transformer(config)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainer = Trainer(model, loss_fn, optimizer)\n",
    "trainer.train(src, tgt, max_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['september 27, 1994'] / ['1994-09-27']\n",
      "['august 19, 2003'] / ['2003-08-19']\n"
     ]
    }
   ],
   "source": [
    "for i in src[:2]:\n",
    "    print(str(tokenizer.to_string(i.view(1,29))),end=\" / \")\n",
    "    print( greedy_decoding(model,i.view(1,29),\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 19, 36, 19, 24, 25,  6,  1,  2,  8,  6,  1,  4,  4, 16]])\n",
      "torch.Size([1, 15]) torch.Size([50000, 29])\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11, 19, 36, 19,\n",
      "         24, 25,  6,  1,  2,  8,  6,  1,  4,  4, 16]]) \n",
      " torch.Size([1, 29]) torch.Size([50000, 29])\n",
      "['august 10, 1994']\n",
      "['1994-08-10']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input=tokenizer.to_tensor(\"august 10, 1994\").view(1,-1)\n",
    "print(input)\n",
    "print(input.shape,src.shape)\n",
    "input = torch.cat([torch.zeros(1, src.size(1) - input.size(1)) , input], dim=1).to(torch.int64)\n",
    "\n",
    "print(input,\"\\n\",input.shape,src.shape)\n",
    "print(str(tokenizer.to_string(input)))\n",
    "output=greedy_decoding(model,input,\"_\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(trm, src, start_token):\n",
    "    N = src.size(0)\n",
    "    preds = tokenizer.to_tensor([start_token]*N)\n",
    "\n",
    "    for _ in range(10):\n",
    "        y_pred,a,b,c = trm(src,preds)\n",
    "        t_pred = torch.argmax(y_pred[:,-1,:], axis=-1, keepdims=True)\n",
    "        preds = torch.cat([preds, t_pred], axis=1)            \n",
    "    \n",
    "    return tokenizer.to_string(preds[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
