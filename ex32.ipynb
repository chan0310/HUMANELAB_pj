{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Seq2SeqModel.Seq2SeqModel import Seq2SeqModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'New Action', 'fr': 'Nouvelle action'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][3][\"translation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "path=\"savedModel/saved1\"\n",
    "tokenizer_checkpoint =path + \"/tokenizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/pytorchVenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3635: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(fr_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Par', '▁dé', 'f', 'aut', ',', '▁dé', 've', 'lop', 'per', '▁les', '▁fil', 's', '▁de', '▁discussion', '</s>']\n",
      "['▁Par', '▁défaut', ',', '▁développer', '▁les', '▁fils', '▁de', '▁discussion', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"<bos> \"+ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [\"<bos> \"+ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # 타겟을 위한 토크나이저 셋업\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "validation_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "X_train = train_dataset[\"input_ids\"]\n",
    "y_train = train_dataset[\"labels\"]\n",
    "\n",
    "X_val = validation_dataset[\"input_ids\"]\n",
    "y_val = validation_dataset[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59515"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<bos>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> ▁Calibration is▁about to check the▁value▁range your▁device▁delivers.▁Please▁move▁axis %1 %2 on your▁device to the maximum position. Press▁any▁button on the▁device or▁click on the'Next 'button to continue with the▁next▁step.\n",
      "<bos> Le calibrage va vérifier la plage de valeurs que votre matériel produit. Veuillez déplacer l'axe %1 %2 de votre périphérique à la position maximale. Appuyez sur n'importe quel bouton du périphérique ou sur le bouton « & #160; Suivant & #160; » pour la prochaine étape.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string([i for i in tokenizer.convert_ids_to_tokens(X_train[0]) if i != tokenizer.eos_token]))\n",
    "print(tokenizer.convert_tokens_to_string([i for i in tokenizer.convert_ids_to_tokens(y_train[0]) if i != tokenizer.eos_token]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Transformer.Config import Config\n",
    "from Transformer.Model import Transformer\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "config=Config(3)\n",
    "model=Transformer(config)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "batchsize=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "module=Seq2SeqModel( tokenizer,optimizer,loss_fn,\n",
    "                    X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_enc_vocab': 59515,\n",
       " 'n_dec_vocab': 59515,\n",
       " 'n_enc_seq': 128,\n",
       " 'n_dec_seq': 128,\n",
       " 'n_layer': 2,\n",
       " 'd_hidn': 128,\n",
       " 'i_pad': 0,\n",
       " 'd_ff': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'layer_norm_epsilon': 1e-12}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> ▁Default to▁expanded▁threads</s>\n",
      "=======\n",
      "['<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <bos> ▁Default to▁expanded▁threads', '<bos> *. ui *. UI_BAR_User Interface▁Files', '<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <bos> New Action']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string([i for i in tokenizer.convert_ids_to_tokens(X_train[1]) ]))\n",
    "print(\"=======\")\n",
    "print(module.seq_list_to_text_list(X_train[1:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5   Batch: 4/4   cost: 11.5331: |███████████████     |75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Running_Loss: 11.536 VAL_ACC: 0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|                    |0.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m module\u001b[39m.\u001b[39;49mtrain_main(\u001b[39m5\u001b[39;49m,\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/HUMANE lab/project/transformer_impl/Seq2SeqModel/Seq2SeqModel.py:81\u001b[0m, in \u001b[0;36mSeq2SeqModel.train_main\u001b[0;34m(self, max_epoch, check)\u001b[0m\n\u001b[1;32m     77\u001b[0m epT\u001b[39m=\u001b[39mtqdm(\u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data_loader),bar_format\u001b[39m=\u001b[39mbar_format,\n\u001b[1;32m     78\u001b[0m          total\u001b[39m=\u001b[39mtotal,unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatches\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m i, (x1, x2, yy) \u001b[39min\u001b[39;00m epT:\n\u001b[0;32m---> 81\u001b[0m     current_loss,attentions\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_each_batch(x1,x2,yy)\n\u001b[1;32m     82\u001b[0m     running_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mcurrent_loss\n\u001b[1;32m     83\u001b[0m     epT\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmax_epoch\u001b[39m}\u001b[39;00m\u001b[39m   Batch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtotal\u001b[39m}\u001b[39;00m\u001b[39m   cost: \u001b[39m\u001b[39m{\u001b[39;00mcurrent_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/HUMANE lab/project/transformer_impl/Seq2SeqModel/Seq2SeqModel.py:58\u001b[0m, in \u001b[0;36mSeq2SeqModel.train_each_batch\u001b[0;34m(self, x1, x2, yy)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_each_batch\u001b[39m(\u001b[39mself\u001b[39m,x1,x2,yy):\n\u001b[0;32m---> 58\u001b[0m         y_pred,ea,de,eda \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x1, x2)\n\u001b[1;32m     59\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(y_pred\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, y_pred\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), yy\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     60\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pytorchVenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/HUMANE lab/project/transformer_impl/Transformer/Model.py:19\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, enc_inputs, dec_inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m dec_outputs, dec_self_attn_probs, dec_enc_attn_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(dec_inputs, enc_inputs, enc_outputs)\n\u001b[1;32m     18\u001b[0m \u001b[39m# (batchs, n_dec_seq, n_dec_vocab)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m dec_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprojection(dec_outputs)\n\u001b[1;32m     20\u001b[0m \u001b[39m# (batchs, n_dec_seq, n_dec_vocab), [(batchs, n_head, n_enc_seq, n_enc_seq)], [(batchs, n_head, n_dec_seq, n_dec_seq)], [(batchs, n_head, n_dec_seq, n_enc_seq)]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pytorchVenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/pytorchVenv/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "module.train_main(5,4) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
